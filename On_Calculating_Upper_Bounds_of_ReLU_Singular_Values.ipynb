{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Calculating Upper Bounds of ReLU Singular Values\n",
    "If you use this resource in a scientific publication, we would appreciate references to the following paper: TODO\n",
    "\n",
    "\n",
    "## What are we doing?\n",
    "We aim to calculate upper bounds of data dependent ReLU singular values.\n",
    "\n",
    "## What is a ReLU singular value?\n",
    "Let $A\\in\\mathbb{R}^{m\\times n}$ and $b\\in\\mathbb{R}^m$, we then define the $k$th ReLU singular value of the operator $\\text{ReLU}(A\\cdot+b)$ over the set $X\\subset\\mathbb{R}^m$ as\n",
    "\n",
    "$s_{k, X} = \\min_{\\text{rank}(L)\\le k}\\max_{x\\in X} ||\\text{ReLU}(Ax+b)-\\text{ReLU}(Lx+b)||_2.$\n",
    "\n",
    "\n",
    "## Approach/Algorithm\n",
    "The calculation is a two step process:\n",
    "\n",
    "1. Approximate\n",
    "\n",
    "    $W_\\ast, M_\\ast = \\displaystyle\\text{argmin}_{\\substack{W \\in \\mathbb{R}^{m\\times k}\\\\M\\in \\mathbb{R}^{k \\times n}}} \\sum_{x\\in X} ||\\text{ReLU}(Ax + b)-\\text{ReLU}(WMx + b)||_2^2$,\n",
    "   \n",
    "    e.g. via some kind of stochastic gradient descent (in our case Adam).\n",
    "2. Calculate the approximate upper bound via\n",
    "\n",
    "    $s_{k, X} \\le \\max_{x\\in X}||\\text{ReLU}(Ax + b)-\\text{ReLU}(W_\\ast M_\\ast x + b)||_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_relu_layer_low_rank(A, b, xs_train, rank, batch_size=-1, log_loss_every_n_batches=100):\n",
    "    \"\"\"\n",
    "    Calculates a low rank approximation of rank k of the ReLU layer ReLU(Ax+b)\n",
    "    w.r.t. the data set xs_train. I.e. an approximation of the form ReLU(WMx+b),\n",
    "    where (for A of the shape m x n) W of the shape m x rank and\n",
    "    M of the shape rank x n.\n",
    "\n",
    "    Keyword arguments:\n",
    "    A -- The weight matrix of the layer.\n",
    "    b -- The bias of the layer.\n",
    "    xs_train -- The data set w.r.t. which we calculate the ReLU singular value, given as a matrix where each column represents a data point.\n",
    "    rank -- The rank of the approximation.\n",
    "    batch_size -- The batch size with which we optimize for the approximation. -1 means: batch = full data set.\n",
    "    log_loss_every_n_batches -- How often do we evaluate our progress?\n",
    "    \"\"\"\n",
    "    # Set the batch size used for the optimization.\n",
    "    if batch_size == -1:\n",
    "        batch_size = xs_train.shape[1]\n",
    "\n",
    "    # Calculate output of the layer to train approximation based on it.\n",
    "    ys_train = np.clip(np.dot(A, xs_train) + b, 0, np.inf)\n",
    "\n",
    "    # Calculate inital guess for the low rank approximation via a truncated SVD.\n",
    "    W, d, M = np.linalg.svd(A)\n",
    "    d = np.sqrt(d)\n",
    "    for i in range(len(d)):\n",
    "        W[:,i] *= d[i]\n",
    "        M[i,:] *= d[i]\n",
    "    rank = np.min([rank, len(d)])\n",
    "    M = M[:rank,:]\n",
    "    W = W[:,:rank]\n",
    "\n",
    "    # Define Tensorflow variables for the approximation.\n",
    "    W_tf = tf.Variable(W, dtype=tf.float64)\n",
    "    M_tf = tf.Variable(M, dtype=tf.float64)\n",
    "    b_tf = tf.Variable(b, dtype=tf.float64, trainable=False)\n",
    "\n",
    "    # Define in- and output placeholders for training.\n",
    "    xs = tf.placeholder(dtype=tf.float64)\n",
    "    ys = tf.placeholder(dtype=tf.float64)\n",
    "\n",
    "    # Define computational structure, i.e. the one layer \"neural net\".\n",
    "    ys_predict = tf.nn.relu(tf.matmul(tf.matmul(W_tf, M_tf), xs) + b_tf)\n",
    "\n",
    "    # Define loss-function: simple least squares\n",
    "    loss = tf.reduce_sum((ys-ys_predict)**2)\n",
    "\n",
    "    # Define optimizer \n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_step = optimizer.minimize(loss)\n",
    "    \n",
    "    # Close possibly still active TF sessions and start an interactive one.\n",
    "    tf.InteractiveSession().close()\n",
    "    sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # Training/Optimizing for the approximation.\n",
    "    loss_s = []\n",
    "    i = -1\n",
    "    while True:\n",
    "        i += 1\n",
    "        # Create batch.\n",
    "        idxs = (np.random.rand(batch_size)*xs_train.shape[1]).astype(int)\n",
    "        batch_xs = xs_train[:,idxs]\n",
    "        batch_ys = ys_train[:,idxs]\n",
    "\n",
    "        # Make training step\n",
    "        sess.run(train_step, feed_dict={xs: batch_xs, ys:batch_ys})\n",
    "\n",
    "        # Check whether it's time to log our loss/progress.\n",
    "        if i % log_loss_every_n_batches == 0:\n",
    "            loss_train = sess.run(loss, feed_dict={xs:xs_train, ys:ys_train})\n",
    "            loss_s += [np.sqrt(loss_train/xs_train.shape[1])]\n",
    "\n",
    "        # If the current approximation is the best one yet, save it.\n",
    "        if np.min(loss_s) == loss_s[-1]:\n",
    "            W_best, M_best = sess.run(W_tf), sess.run(M_tf)\n",
    "\n",
    "        # Well, this was pointless.\n",
    "        if rank == 0:\n",
    "            break\n",
    "\n",
    "        # Heuristic to determine whether it makes sense to stop optimizing:\n",
    "        # Every beginning is hard -- keep going.\n",
    "        if i > log_loss_every_n_batches*10:\n",
    "            # Have we struggled lately to improve?\n",
    "            struggled = np.sum((np.array(loss_s[-11:-1])-np.array(loss_s[-10:]))>0) < 5 # Less than 5 times decreased the loss over the last 10 iterations?\n",
    "            # And are we no longer in peek shape?\n",
    "            not_peek_shape = np.min(loss_s) <= loss_s[-1]\n",
    "            # If we struggle to improve and are no longer in peek shape, despied the fact that we have tried for a while, than maybe it's time to quit.\n",
    "            if struggled and not_peek_shape:\n",
    "                break\n",
    "\n",
    "    # Get and log final loss.\n",
    "    loss_train = sess.run(loss, feed_dict={xs:xs_train, ys:ys_train})\n",
    "    loss_s += [np.sqrt(loss_train/xs_train.shape[1])]\n",
    "\n",
    "    # Close Tensorflow session.\n",
    "    tf.reset_default_graph()\n",
    "    sess.close()\n",
    "\n",
    "    return (W_best, M_best), loss_s\n",
    "\n",
    "def relu_singular_value_via_approximation(A, b, xs, W, M):\n",
    "    \"\"\"\n",
    "    Calculates an upper bound to ReLU singular value w.r.t. the data set xs\n",
    "    of the layer ReLU(Ax+b) based on the low rank approximation ReLU(WMx+b).\n",
    "\n",
    "    Keyword arguments:\n",
    "    A -- The weight matrix of the layer.\n",
    "    b -- The bias of the layer.\n",
    "    xs -- The data set w.r.t. which we calculate the ReLU singular value.\n",
    "    W -- First of the matrices producing the low rank approximation / bottleneck.\n",
    "    M -- Second of the matrices producing the low rank approximation / bottleneck.\n",
    "    \"\"\"\n",
    "    # calculate output of layer\n",
    "    output_layer = np.clip(np.dot(A,xs)+b, 0, np.inf)\n",
    "    # calculate output of the low rank approximation to the layer\n",
    "    output_approximation = np.clip(np.dot(np.dot(W,M),xs)+b, 0, np.inf)\n",
    "    # calulate and return upper bound of ReLU singular value\n",
    "    return np.max( np.linalg.norm(output_layer - output_approximation, axis=0) )\n",
    "\n",
    "def relu_singular_value(A, b, xs, k, batch_size=-1, log_loss_every_n_batches=100):\n",
    "    \"\"\"\n",
    "    Calculates an upper bound of the k-th ReLU singular value w.r.t. the data set xs\n",
    "    of the layer ReLU(Ax+b).\n",
    "\n",
    "    Keyword arguments:\n",
    "    A -- The weight matrix of the layer.\n",
    "    b -- The bias of the layer.\n",
    "    xs -- The data set w.r.t. which we calculate the ReLU singular value.\n",
    "    k -- Index of singular value, starting at 0!\n",
    "    batch_size -- The batch size with which we optimize for the underlying approximation. -1 means: batch = full data set.\n",
    "    log_loss_every_n_batches -- How often do we evaluate our progress in the underlying approximation?\n",
    "    \"\"\"\n",
    "    # Get low rank approximation.\n",
    "    (W, M), _ = fit_relu_layer_low_rank(A, b, xs, k, batch_size=batch_size, log_loss_every_n_batches=log_loss_every_n_batches)\n",
    "    # calulate and return upper bound of ReLU singular value\n",
    "    return relu_singular_value_via_approximation(A, b, xs, W, M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Example\n",
    "As an example we want to calculate an upper bound for the $k$th ReLU singular value of the layer $\\text{ReLU}(Ax+b)$ over the dataset $X$.\n",
    "\n",
    "Here $A\\in\\mathbb{R}^{n\\times n}$ with entries sampled from $\\mathcal{N}(0,1_{n})$, $b=0$ and $X$ is given by $N$ random vectors with entries sampled from the uniform distribution over the interval $[0, 1]$.\n",
    "\n",
    "Once for $n = 10$, $k = 5$ and $N=1,000$ and once for $n = 50$, $k = 20$ and $N=100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5th ReLU singular value of the layer ReLU(Ax+b), w.r.t. the data set X, is at most 1.31384284081892.\n"
     ]
    }
   ],
   "source": [
    "A = np.random.normal(0, 1, [10, 10])\n",
    "b = np.zeros([10, 1])\n",
    "xs = np.random.rand(10, 10000)\n",
    "\n",
    "bound_relu_singular_value_5 = relu_singular_value(A, b, xs, 5, log_loss_every_n_batches=10)\n",
    "print(f\"The 5th ReLU singular value of the layer ReLU(Ax+b), w.r.t. the data set X, is at most {bound_relu_singular_value_5}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 20th ReLU singular value of the layer ReLU(Ax+b), w.r.t. the data set X, is at most 3.8578114116420457.\n"
     ]
    }
   ],
   "source": [
    "A = np.random.normal(0, 1, [50, 50])\n",
    "b = np.zeros([50, 1])\n",
    "xs = np.random.rand(50, 100)\n",
    "\n",
    "bound_relu_singular_value_20 = relu_singular_value(A, b, xs, 20, log_loss_every_n_batches=10)\n",
    "print(f\"The 20th ReLU singular value of the layer ReLU(Ax+b), w.r.t. the data set X, is at most {bound_relu_singular_value_20}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
